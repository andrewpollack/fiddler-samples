{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will walk you through the basic onboarding steps required to use Fiddler for production model monitoring and explainability. API documentation can be found [here](https://docs.fiddler.ai/api-reference/python-package/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One: Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will install the [Fiddler Python package](https://pypi.org/project/fiddler-client/) and establish an API connection to our Fiddler instance.\n",
    "\n",
    "This Python client is a powerful way to:\n",
    "- Upload the dataset and model to Fiddler\n",
    "- Ingest production events to Fiddler\n",
    "\n",
    "This can be done from a Jupyter Notebook or any python editor that you use to load data and build models.\n",
    "\n",
    "<img src=\"images/qs_d1.png\" width=700 height=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The `url`: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact us if you don’t have it\n",
    "- The `org_id`: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=800 />\n",
    "- The `auth_token`: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=800 />\n",
    "\n",
    "You can also save this config as a file called `fiddler.ini` in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n",
    "<img src=\"images/fiddler_ini.png\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiddler-client==0.6.8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fiddler.ini\n",
    "\n",
    "[FIDDLER]\n",
    "url = http://host.docker.internal:4100\n",
    "org_id = onebox\n",
    "auth_token = YOUR_TOKEN_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddler has three primary constructs, namely projects, datasets and models. This diagram illustrates the relationship between the three.\n",
    "<img src=\"images/qs_d2.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fiddler client provides a number of methods. API documentation can be found [here](https://docs.fiddler.ai/api-reference/python-package/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two: Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case.\n",
    "\n",
    "For the purposes of a full quick start, it is best to create a `project_id` with a unique name to best track your progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'quickstart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: Upload Baseline Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will upload the datasets that will serve as baselines for various product capabilities, including monitoring of model performance, prediction & feature drift, and data errors; generating prediction-level (point) and model-level (global) explanations; and calculating various bias metrics.\n",
    "\n",
    "We recommend using the model's training and test set for the most faithful and actionable metrics. In addition to the model's features and labels, Fiddler requires a few additional attributes to unlock its full suite of capabilities:\n",
    "\n",
    "*   Model predictions (Mandatory: serves as a baseline for prediction drift)\n",
    "*   Model decisions* (Optional: used to monitor model decsions over time, e.g. loan approved vs denied. The data uploaded initially can be random)\n",
    "*  Model metadata* (Any additional fields relevant for model analysis. In the event you intend to use Fiddler to detect model bias, include any relevant protected attributes here, e.g. gender, race, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data you are going to use for training your model. For this tutorial, we will be using an auto insurance dataset that can be found [here](https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/data.csv')\n",
    "df = df.rename(columns={\"State\": \"Location State\"})\n",
    "\n",
    "# Adding a decision column to our data. In this case, we deem a 'high_value' customer as\n",
    "# one with customer_lifetime_value >= 5000\n",
    "df.columns = [x.lower().replace(' ', '_') for x in df.columns]\n",
    "df = df.assign(high_value=['Yes' if x >= 5000 else 'No' for x in df['customer_lifetime_value']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train/Test\n",
    "\n",
    "Now we will split our dataset into a train/test set to be used in training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn==0.21.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = df.sample(frac=0.8,random_state=200)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a model, you first need to upload a sample of the data of the model’s inputs, targets, and additional metadata that might be useful for model analysis. This data sample helps us (among other things) to infer the model schema and the data types and values range of each feature.\n",
    "- This sample has to be a flat table that can be loaded as a pandas DF (```upload_dataset()```).\n",
    "- This input data sample is used for many downstream functions in Fiddler, including: shapley value methods, what-if (ICE) plots, PDP plots, drift, outliers, and data integrity.\n",
    "- We suggest uploading a sample of the model’s training data as it’s the most meaningful for the tasks listed above. For example, model outliers should be ideally based on the training data as that’s the data the model has seen. \n",
    "- You can upload multiple datasets with string identifiers, but we currently do not ascribe any meaning to those. For example: ```dataset={'data': df}``` or ```dataset={'train': train_df, 'test': test_df}```.\n",
    "- Currently we support two input types:\n",
    "    - Tabular\n",
    "    - Single string text, meaning text data in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'auto_insurance'\n",
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a schema for our dataset, and upload the dataset to Fiddler.\n",
    "\n",
    "For the purposes of this tutorial, we have already used the model we will be training in a later step to create **predictions** for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_id not in client.list_datasets():\n",
    "    df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'train': df_train,\n",
    "                 'test': df_test},\n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)\n",
    "else:\n",
    "    df_schema = client.get_dataset_info(dataset_id)\n",
    "\n",
    "df_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Four: Create and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must have noted, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'customer_lifetime_value'\n",
    "continuous_features = ['income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception',\n",
    "                        'number_of_open_complaints', 'number_of_policies', 'total_claim_amount']\n",
    "categorical_features = ['location_state', 'employmentstatus', 'policy_type', 'policy', 'vehicle_class','vehicle_size']\n",
    "\n",
    "feature_columns = list(continuous_features + categorical_features)\n",
    "metadata_cols = ['gender']\n",
    "decision_cols = ['high_value']\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(dataset_id),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    metadata_cols=metadata_cols,\n",
    "    decision_cols=decision_cols,\n",
    "    display_name='Gradient Boosting Regressor',\n",
    "    description='this is a GradientBoostingRegressor model from the tutorial',\n",
    ")\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train your model. For this model, we will be creating a Pipeline that will transform the data passed in, and then run that data through a gradient boosting regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# https://songxia-sophia.medium.com/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea\n",
    "# https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sklearn.pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "target = 'customer_lifetime_value'\n",
    "continuous_features = ['income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception',\n",
    "                        'number_of_open_complaints', 'number_of_policies', 'total_claim_amount']\n",
    "categorical_features = ['location_state', 'employmentstatus', 'policy_type', 'policy', 'vehicle_class','vehicle_size']\n",
    "\n",
    "category_transformer = sklearn.pipeline.Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', category_transformer, categorical_features)])\n",
    "\n",
    "model = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                  n_estimators=100,\n",
    "                                  max_depth=7)\n",
    "model_pipeline = sklearn.pipeline.Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                                  ('model', model)])\n",
    "\n",
    "model_pipeline.fit(df_train.loc[:, df_train.columns !=  target], df_train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Five: Create Model Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we need to save the model and any pre-processing step you had on the input features (for example Categorical encoder, Tokenization, ...).  \n",
    "We currently support the following stored model formats:\n",
    "- For sklearn API based models, pickled models, or any storage format that you can load in the package.py (details below).\n",
    "- For TF, we support TF Saved Model and Keras .h5   \n",
    "\n",
    "Note:\n",
    "- Keras models have to have their input tensor differentiable if Integrated Gradients support is desired\n",
    "- We also need to save the data preprocessing pipeline code, if any. This will be accessed in the package.py\n",
    "\n",
    "In total, we will have a `model.yaml`, a `model.pkl`, and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "model_id = 'gradient_boosting_regressor'\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and schema (`model.pkl` and `model.yaml`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model schema file to model directory\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Saving model\n",
    "with open(model_dir / 'model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(model_pipeline, pkl_file)\n",
    "\n",
    "# Saving schema\n",
    "with open(model_dir / 'model.yaml', 'w') as yaml_file:\n",
    "    yaml.dump({'model': model_info.to_dict()}, yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `package.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`package.py` is a Python module that\n",
    "\n",
    "- Facilitates model loading\n",
    "- Implements interfaces necessary for the Fiddler platform to interact with models.\n",
    "\n",
    "This provides the flexibility to enable Fiddler to support a wide variety of complex models. More information can be found [here](https://docs.fiddler.ai/api-reference/package-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gradient_boosting_regressor/package.py\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "TARGET = 'customer_lifetime_value'\n",
    "PREDICTION = 'predicted_customer_lifetime_value'\n",
    "\n",
    "class GBRegressor:\n",
    "    \"\"\"A Gradient Boosting Regressor predictor for auto_insurance data.\n",
    "       This loads the predictor once and runs for each call to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path, output_column=None):\n",
    "        \"\"\"\n",
    "        :param model_path: The directory where the model is saved.\n",
    "        :param output_column: list of column name(s) for the output.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.output_column = output_column\n",
    "\n",
    "        file_path = os.path.join(self.model_path, 'model.pkl')\n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.model = pkl.load(file)\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        return pd.DataFrame(\n",
    "            self.model.predict(input_df.loc[:, input_df.columns != TARGET]), \n",
    "            columns=self.output_column\n",
    "        )\n",
    "\n",
    "def get_model():\n",
    "    return GBRegressor(model_path=PACKAGE_PATH, output_column=[PREDICTION])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Six: Upload Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Package\n",
    " \n",
    "This step finds issues with the `package.py` composed above to enable easy debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler import PackageValidator\n",
    "\n",
    "validator = PackageValidator(model_info, df_schema, model_dir)\n",
    "passed, errors = validator.run_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the [upload_model_package](https://docs.fiddler.ai/api-reference/python-package/#upload-model-package) to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset` which the model is linked to (optional)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first delete the model if it already exists in the project\n",
    "if model_id in client.list_models(project_id):\n",
    "    client.delete_model(project_id, model_id)\n",
    "    print('Model deleted')\n",
    "    \n",
    "client.upload_model_package(artifact_path=model_dir, project_id=project_id, model_id=model_id)\n",
    "f\"Project '{project_id} now contains model '{model_id}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's trigger the model to run its predictions by calling the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.trigger_model_predictions(project_id, model_id, dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test out our model by interfacing with the client and calling [run model](https://docs.fiddler.ai/api-reference/python-package/#run-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = df_train[0: 10]\n",
    "result = client.run_model(project_id, model_id, prediction_input, log_events=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Seven: Simulate Monitoring Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will be simulating traffic to send for our model monitoring by using [publish_event](https://docs.fiddler.ai/api-reference/python-package/#publish-event). This will be the equivalent of running our model separately on some data, and either sending to Fiddler then, or saving this information to a log and sending at a later point.\n",
    "\n",
    "For this demonstration, we will be going with a log-related approach. This log will contain rows that have inputs, predictions, and real targets. To most accurately simulate this as a time-series event, we will also be calling a function to generate a timestamp in the last 2 weeks. Real data will ideally have a timestamp related to when the event took place; otherwise, the current time will be used.\n",
    "\n",
    "**Note**: The timestamp must be in UTC milliseconds. See [here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from random import sample, randint\n",
    "NUM_EVENTS_TO_SEND = 50\n",
    "\n",
    "def getTimestampFromPastTwoWeeks():\n",
    "    \"\"\"\n",
    "    Generate a randomized timestamp from the past two weeks. Timestamp is in \n",
    "    milliseconds since epoch in UTC.\n",
    "    \"\"\"\n",
    "    TWO_WEEKS_MS = 604800 * 2 * 1000\n",
    "    current_time_in_ms = round(time.time() * 1000)\n",
    "    \n",
    "    random_time_in_past_two_weeks = current_time_in_ms - randint(0, TWO_WEEKS_MS)\n",
    "    return random_time_in_past_two_weeks\n",
    "\n",
    "        \n",
    "event_log = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/event_log.csv')\n",
    "\n",
    "# Convert this dataframe into a list of dictionary events, where each event is its own dictionary\n",
    "event_list_dict = event_log.sample(n=NUM_EVENTS_TO_SEND).to_dict(orient='records') \n",
    "\n",
    "for ind, event_dict in enumerate(event_list_dict):\n",
    "    event_ms_time_stamp = getTimestampFromPastTwoWeeks()\n",
    "    result = client.publish_event(project_id, model_id, event_dict, event_time_stamp=event_ms_time_stamp)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    readable_timestamp = datetime.datetime.fromtimestamp(event_ms_time_stamp/1000.0)\n",
    "    \n",
    "    print(f'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \\n{readable_timestamp} UTC: \\n{event_dict}')\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the case that labels are ingested in a future point, an event can be updated by calling:\n",
    "\n",
    "- `res = fiddler_api.publish_event(project_id, model_id, event, event_id: customer, update_event=True, event_time_stamp=row['__occurred_at'])`\n",
    "\n",
    "By setting the `update_event` flag to be true, the event identifed by `event_id` will be updated with whatever additional information you pass in through `event`, including a target label. See [here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
