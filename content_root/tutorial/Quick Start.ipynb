{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will walk you through the basic onboarding steps required to use Fiddler for production model monitoring and explainability. API documentation can be found [here](https://docs.fiddler.ai/api-reference/python-package/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One: Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will install the [Fiddler Python package](https://pypi.org/project/fiddler-client/) and establish an API connection to our Fiddler instance.\n",
    "\n",
    "This Python client is a powerful way to:\n",
    "- Upload the dataset and model to Fiddler\n",
    "- Ingest production events to Fiddler\n",
    "\n",
    "This can be done from a Jupyter Notebook or any python editor that you use to load data and build models.\n",
    "\n",
    "<img src=\"images/fiddler_client.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The `url`: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact us if you don’t have it\n",
    "- The `org_id`: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=800 />\n",
    "- The `auth_token`: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=800 />\n",
    "\n",
    "You can also save this config as a file called `fiddler.ini` in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n",
    "<img src=\"images/fiddler_ini.png\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fiddler-client==0.6.8 in /opt/conda/lib/python3.7/site-packages (0.6.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fiddler-client==0.6.8) (5.3.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fiddler-client==0.6.8) (0.25.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fiddler-client==0.6.8) (2.22.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->fiddler-client==0.6.8) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->fiddler-client==0.6.8) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas->fiddler-client==0.6.8) (1.17.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fiddler-client==0.6.8) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fiddler-client==0.6.8) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fiddler-client==0.6.8) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fiddler-client==0.6.8) (2019.11.28)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->fiddler-client==0.6.8) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fiddler-client==0.6.8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fiddler.ini\n"
     ]
    }
   ],
   "source": [
    "%%writefile fiddler.ini\n",
    "\n",
    "[FIDDLER]\n",
    "url = http://host.docker.internal:4100\n",
    "org_id = onebox\n",
    "auth_token = 1rkqdQ56CKsU-QPrnhFqQz8EHbb7kpht1XxPj8RdOMw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddler has three primary constructs, namely projects, datasets and models. This diagram illustrates the relationship between the three.\n",
    "<img src=\"images/projects_data_models.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fiddler client provides a number of methods. API documentation can be found [here](https://docs.fiddler.ai/api-reference/python-package/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two: Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case.\n",
    "\n",
    "For the purposes of a full quick start, it is best to create a `project_id` with a unique name to best track your progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tutorial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: Upload Baseline Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will upload the datasets that will serve as baselines for various product capabilities, including monitoring of model performance, prediction & feature drift, and data errors; generating prediction-level (point) and model-level (global) explanations; and calculating various bias metrics.\n",
    "\n",
    "We recommend using the model's training and test set for the most faithful and actionable metrics. In addition to the model's features and labels, Fiddler requires a few additional attributes to unlock its full suite of capabilities:\n",
    "\n",
    "*   Model predictions (Mandatory: serves as a baseline for prediction drift)\n",
    "*   Model decisions* (Optional: used to monitor model decsions over time, e.g. loan approved vs denied. The data uploaded initially can be random)\n",
    "*  Model metadata* (Any additional fields relevant for model analysis. In the event you intend to use Fiddler to detect model bias, include any relevant protected attributes here, e.g. gender, race, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data you are going to use for training your model. For this tutorial, we will be using an auto insurance dataset that can be found [here](https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>location_state</th>\n",
       "      <th>customer_lifetime_value</th>\n",
       "      <th>response</th>\n",
       "      <th>coverage</th>\n",
       "      <th>education</th>\n",
       "      <th>effective_to_date</th>\n",
       "      <th>employmentstatus</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>...</th>\n",
       "      <th>months_since_policy_inception</th>\n",
       "      <th>number_of_open_complaints</th>\n",
       "      <th>number_of_policies</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>policy</th>\n",
       "      <th>renew_offer_type</th>\n",
       "      <th>sales_channel</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>vehicle_class</th>\n",
       "      <th>vehicle_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BU79786</td>\n",
       "      <td>Washington</td>\n",
       "      <td>2763.519279</td>\n",
       "      <td>No</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2/24/11</td>\n",
       "      <td>Employed</td>\n",
       "      <td>F</td>\n",
       "      <td>56274</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Corporate Auto</td>\n",
       "      <td>Corporate L3</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Agent</td>\n",
       "      <td>384.811147</td>\n",
       "      <td>Two-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QZ44356</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>6979.535903</td>\n",
       "      <td>No</td>\n",
       "      <td>Extended</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>1/31/11</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Personal Auto</td>\n",
       "      <td>Personal L3</td>\n",
       "      <td>Offer3</td>\n",
       "      <td>Agent</td>\n",
       "      <td>1131.464935</td>\n",
       "      <td>Four-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI49188</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>12887.431650</td>\n",
       "      <td>No</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2/19/11</td>\n",
       "      <td>Employed</td>\n",
       "      <td>F</td>\n",
       "      <td>48767</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Personal Auto</td>\n",
       "      <td>Personal L3</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Agent</td>\n",
       "      <td>566.472247</td>\n",
       "      <td>Two-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WW63253</td>\n",
       "      <td>California</td>\n",
       "      <td>7645.861827</td>\n",
       "      <td>No</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>1/20/11</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Corporate Auto</td>\n",
       "      <td>Corporate L2</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Call Center</td>\n",
       "      <td>529.881344</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HB64268</td>\n",
       "      <td>Washington</td>\n",
       "      <td>2813.692575</td>\n",
       "      <td>No</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2/3/11</td>\n",
       "      <td>Employed</td>\n",
       "      <td>M</td>\n",
       "      <td>43836</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Personal Auto</td>\n",
       "      <td>Personal L1</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Agent</td>\n",
       "      <td>138.130879</td>\n",
       "      <td>Four-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer location_state  customer_lifetime_value response  coverage  \\\n",
       "0  BU79786     Washington              2763.519279       No     Basic   \n",
       "1  QZ44356        Arizona              6979.535903       No  Extended   \n",
       "2  AI49188         Nevada             12887.431650       No   Premium   \n",
       "3  WW63253     California              7645.861827       No     Basic   \n",
       "4  HB64268     Washington              2813.692575       No     Basic   \n",
       "\n",
       "  education effective_to_date employmentstatus gender  income  ...  \\\n",
       "0  Bachelor           2/24/11         Employed      F   56274  ...   \n",
       "1  Bachelor           1/31/11       Unemployed      F       0  ...   \n",
       "2  Bachelor           2/19/11         Employed      F   48767  ...   \n",
       "3  Bachelor           1/20/11       Unemployed      M       0  ...   \n",
       "4  Bachelor            2/3/11         Employed      M   43836  ...   \n",
       "\n",
       "  months_since_policy_inception number_of_open_complaints  number_of_policies  \\\n",
       "0                             5                         0                   1   \n",
       "1                            42                         0                   8   \n",
       "2                            38                         0                   2   \n",
       "3                            65                         0                   7   \n",
       "4                            44                         0                   1   \n",
       "\n",
       "      policy_type        policy  renew_offer_type  sales_channel  \\\n",
       "0  Corporate Auto  Corporate L3            Offer1          Agent   \n",
       "1   Personal Auto   Personal L3            Offer3          Agent   \n",
       "2   Personal Auto   Personal L3            Offer1          Agent   \n",
       "3  Corporate Auto  Corporate L2            Offer1    Call Center   \n",
       "4   Personal Auto   Personal L1            Offer1          Agent   \n",
       "\n",
       "  total_claim_amount  vehicle_class vehicle_size  \n",
       "0         384.811147   Two-Door Car      Medsize  \n",
       "1        1131.464935  Four-Door Car      Medsize  \n",
       "2         566.472247   Two-Door Car      Medsize  \n",
       "3         529.881344            SUV      Medsize  \n",
       "4         138.130879  Four-Door Car      Medsize  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/data.csv')\n",
    "df = df.rename(columns={\"State\": \"Location State\"})\n",
    "df.columns = [x.lower().replace(' ', '_') for x in df.columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train/Test\n",
    "\n",
    "Now we will split our dataset into a train/test set to be used in training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn==0.21.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = df.sample(frac=0.8,random_state=200)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a model, you first need to upload a sample of the data of the model’s inputs, targets, and additional metadata that might be useful for model analysis. This data sample helps us (among other things) to infer the model schema and the data types and values range of each feature.\n",
    "- This sample has to be a flat table that can be loaded as a pandas DF (```upload_dataset()```).\n",
    "- This input data sample is used for many downstream functions in Fiddler, including: shapley value methods, what-if (ICE) plots, PDP plots, drift, outliers, and data integrity.\n",
    "- We suggest uploading a sample of the model’s training data as it’s the most meaningful for the tasks listed above. For example, model outliers should be ideally based on the training data as that’s the data the model has seen. \n",
    "- You can upload multiple datasets with string identifiers, but we currently do not ascribe any meaning to those. For example: ```dataset={'data': df}``` or ```dataset={'train': train_df, 'test': test_df}```.\n",
    "- Currently we support two input types:\n",
    "    - Tabular\n",
    "    - Single string text, meaning text data in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto_insurance'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = 'auto_insurance'\n",
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a schema for our dataset, and upload the dataset to Fiddler.\n",
    "\n",
    "For the purposes of this tutorial, we have already used the model we will be training in a later step to create **predictions** for our model. These predictions will be imported from a separate file, and appended as a column to this dataset. Fiddler utilizes these predictions for **TODO**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo:\n",
       "  display_name: \n",
       "  files: []\n",
       "  columns:\n",
       "                               column     dtype count(possible_values)  \\\n",
       "    0                        customer    STRING                          \n",
       "    1                  location_state  CATEGORY                      5   \n",
       "    2         customer_lifetime_value     FLOAT                          \n",
       "    3                        response  CATEGORY                      2   \n",
       "    4                        coverage  CATEGORY                      3   \n",
       "    5                       education  CATEGORY                      5   \n",
       "    6               effective_to_date  CATEGORY                     59   \n",
       "    7                employmentstatus  CATEGORY                      5   \n",
       "    8                          gender  CATEGORY                      2   \n",
       "    9                          income   INTEGER                          \n",
       "    10                  location_code  CATEGORY                      3   \n",
       "    11                 marital_status  CATEGORY                      3   \n",
       "    12           monthly_premium_auto   INTEGER                          \n",
       "    13        months_since_last_claim   INTEGER                          \n",
       "    14  months_since_policy_inception   INTEGER                          \n",
       "    15      number_of_open_complaints   INTEGER                          \n",
       "    16             number_of_policies   INTEGER                          \n",
       "    17                    policy_type  CATEGORY                      3   \n",
       "    18                         policy  CATEGORY                      9   \n",
       "    19               renew_offer_type  CATEGORY                      4   \n",
       "    20                  sales_channel  CATEGORY                      4   \n",
       "    21             total_claim_amount     FLOAT                          \n",
       "    22                  vehicle_class  CATEGORY                      6   \n",
       "    23                   vehicle_size  CATEGORY                      3   \n",
       "\n",
       "       is_nullable          value_range  \n",
       "    0        False                       \n",
       "    1        False                       \n",
       "    2        False   1,898.0 - 83,330.0  \n",
       "    3        False                       \n",
       "    4        False                       \n",
       "    5        False                       \n",
       "    6        False                       \n",
       "    7        False                       \n",
       "    8        False                       \n",
       "    9        False         0 - 99,980    \n",
       "    10       False                       \n",
       "    11       False                       \n",
       "    12       False        61 - 298       \n",
       "    13       False         0 - 35        \n",
       "    14       False         0 - 99        \n",
       "    15       False         0 - 5         \n",
       "    16       False         1 - 9         \n",
       "    17       False                       \n",
       "    18       False                       \n",
       "    19       False                       \n",
       "    20       False                       \n",
       "    21       False     0.099 - 2,893.0   \n",
       "    22       False                       \n",
       "    23       False                       "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if dataset_id not in client.list_datasets():\n",
    "    df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'train': df_train,\n",
    "                 'test': df_test},\n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)\n",
    "else:\n",
    "    df_schema = client.get_dataset_info(dataset_id)\n",
    "\n",
    "df_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Four: Create and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must have noted, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelInfo:\n",
       "  display_name: Gradient Boosting Regressor\n",
       "  description: this is a GradientBoostingRegressor model from the tutorial\n",
       "  input_type: ModelInputType.TABULAR\n",
       "  model_task: ModelTask.REGRESSION\n",
       "  inputs:\n",
       "                               column     dtype count(possible_values)  \\\n",
       "    0                  location_state  CATEGORY                      5   \n",
       "    1                employmentstatus  CATEGORY                      5   \n",
       "    2                          income   INTEGER                          \n",
       "    3            monthly_premium_auto   INTEGER                          \n",
       "    4         months_since_last_claim   INTEGER                          \n",
       "    5   months_since_policy_inception   INTEGER                          \n",
       "    6       number_of_open_complaints   INTEGER                          \n",
       "    7              number_of_policies   INTEGER                          \n",
       "    8                     policy_type  CATEGORY                      3   \n",
       "    9                          policy  CATEGORY                      9   \n",
       "    10             total_claim_amount     FLOAT                          \n",
       "    11                  vehicle_class  CATEGORY                      6   \n",
       "    12                   vehicle_size  CATEGORY                      3   \n",
       "\n",
       "       is_nullable        value_range  \n",
       "    0        False                     \n",
       "    1        False                     \n",
       "    2        False        0 - 99,980   \n",
       "    3        False       61 - 298      \n",
       "    4        False        0 - 35       \n",
       "    5        False        0 - 99       \n",
       "    6        False        0 - 5        \n",
       "    7        False        1 - 9        \n",
       "    8        False                     \n",
       "    9        False                     \n",
       "    10       False    0.099 - 2,893.0  \n",
       "    11       False                     \n",
       "    12       False                     \n",
       "  outputs:\n",
       "                                  column  dtype count(possible_values)  \\\n",
       "    0  predicted_customer_lifetime_value  FLOAT                          \n",
       "\n",
       "      is_nullable value_range  \n",
       "    0       False       * - *  \n",
       "  metadata:\n",
       "       column     dtype  count(possible_values) is_nullable value_range\n",
       "    0  gender  CATEGORY                       2       False            \n",
       "\n",
       "  targets: [Column(name=\"customer_lifetime_value\", data_type=DataType.FLOAT, possible_values=None, is_nullable=False, value_range_min=1898.007675, value_range_max=83325.38119)]\n",
       "  misc:\n",
       "    {}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'customer_lifetime_value'\n",
    "continuous_features = ['income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception',\n",
    "                        'number_of_open_complaints', 'number_of_policies', 'total_claim_amount']\n",
    "categorical_features = ['location_state', 'employmentstatus', 'policy_type', 'policy', 'vehicle_class','vehicle_size']\n",
    "\n",
    "feature_columns = list(continuous_features + categorical_features)\n",
    "metadata_cols = ['gender']\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(dataset_id),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    metadata_cols=metadata_cols,\n",
    "    display_name='Gradient Boosting Regressor',\n",
    "    description='this is a GradientBoostingRegressor model from the tutorial',\n",
    ")\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train your model. For this model, we will be creating a Pipeline that will transform the data passed in, and then run that data through a gradient boosting regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('onehot',\n",
       "                                                                   OneHotEncoder(categorical_features=None,\n",
       "                                                                                 categories=None,\n",
       "                                                                                 drop=None,\n",
       "                                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 n_values=None,\n",
       "                                                                                 sparse=True))],\n",
       "                                                           verbos...\n",
       "                                           init=None, learning_rate=0.1,\n",
       "                                           loss='ls', max_depth=7,\n",
       "                                           max_features=None,\n",
       "                                           max_leaf_nodes=None,\n",
       "                                           min_impurity_decrease=0.0,\n",
       "                                           min_impurity_split=None,\n",
       "                                           min_samples_leaf=1,\n",
       "                                           min_samples_split=2,\n",
       "                                           min_weight_fraction_leaf=0.0,\n",
       "                                           n_estimators=100,\n",
       "                                           n_iter_no_change=None,\n",
       "                                           presort='auto', random_state=None,\n",
       "                                           subsample=1.0, tol=0.0001,\n",
       "                                           validation_fraction=0.1, verbose=0,\n",
       "                                           warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# references\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# https://songxia-sophia.medium.com/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea\n",
    "# https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sklearn.pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "target = 'customer_lifetime_value'\n",
    "continuous_features = ['income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception',\n",
    "                        'number_of_open_complaints', 'number_of_policies', 'total_claim_amount']\n",
    "categorical_features = ['location_state', 'employmentstatus', 'policy_type', 'policy', 'vehicle_class','vehicle_size']\n",
    "\n",
    "category_transformer = sklearn.pipeline.Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', category_transformer, categorical_features)])\n",
    "\n",
    "model = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                  n_estimators=100,\n",
    "                                  max_depth=7)\n",
    "model_pipeline = sklearn.pipeline.Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                                  ('model', model)])\n",
    "\n",
    "model_pipeline.fit(df_train.loc[:, df_train.columns !=  target], df_train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Five: Create Model Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we need to save the model and any pre-processing step you had on the input features (for example Categorical encoder, Tokenization, ...).  \n",
    "We currently support the following stored model formats:\n",
    "- For sklearn API based models, pickled models, or any storage format that you can load in the package.py (details below).\n",
    "- For TF, we support TF Saved Model and Keras .h5   \n",
    "\n",
    "Note:\n",
    "- Keras models have to have their input tensor differentiable if Integrated Gradients support is desired\n",
    "- We also need to save the data preprocessing pipeline code, if any. This will be accessed in the package.py\n",
    "\n",
    "In total, we will have a `model.yaml`, a `model.pkl`, and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "model_id = 'tutorial_gradient_boosting_regressor'\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and schema (`model.pkl` and `model.yaml`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model schema file to model directory\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Saving model\n",
    "with open(model_dir / 'model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(model_pipeline, pkl_file)\n",
    "\n",
    "# Saving schema\n",
    "with open(model_dir / 'model.yaml', 'w') as yaml_file:\n",
    "    yaml.dump({'model': model_info.to_dict()}, yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `package.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`package.py` is a Python module that\n",
    "\n",
    "- Facilitates model loading\n",
    "- Implements interfaces necessary for the Fiddler platform to interact with models.\n",
    "\n",
    "This provides the flexibility to enable Fiddler to support a wide variety of complex models. More information can be found [here](https://docs.fiddler.ai/api-reference/package-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tutorial_gradient_boosting_regressor/package.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tutorial_gradient_boosting_regressor/package.py\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "TARGET = 'customer_lifetime_value'\n",
    "PREDICTION = 'predicted_customer_lifetime_value'\n",
    "\n",
    "class GBRegressor:\n",
    "    \"\"\"A Gradient Boosting Regressor predictor for auto_insurance data.\n",
    "       This loads the predictor once and runs for each call to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path, output_column=None):\n",
    "        \"\"\"\n",
    "        :param model_path: The directory where the model is saved.\n",
    "        :param output_column: list of column name(s) for the output.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.output_column = output_column\n",
    "\n",
    "        file_path = os.path.join(self.model_path, 'model.pkl')\n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.model = pkl.load(file)\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        return pd.DataFrame(\n",
    "            self.model.predict(input_df.loc[:, input_df.columns != TARGET]), \n",
    "            columns=self.output_column\n",
    "        )\n",
    "\n",
    "def get_model():\n",
    "    return GBRegressor(model_path=PACKAGE_PATH, output_column=[PREDICTION])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Six: Upload Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Package\n",
    " \n",
    "This step finds issues with the `package.py` composed above to enable easy debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Result: \u001b[92mPASS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from fiddler import PackageValidator\n",
    "\n",
    "validator = PackageValidator(model_info, df_schema, model_dir)\n",
    "passed, errors = validator.run_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the [upload_model_package](https://docs.fiddler.ai/api-reference/python-package/#upload-model-package) to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset` which the model is linked to (optional)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project 'tutorials now contains model 'tutorial_gradient_boosting_regressor'\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first delete the model if it already exists in the project\n",
    "if model_id in client.list_models(project_id):\n",
    "    client.delete_model(project_id, model_id)\n",
    "    print('Model deleted')\n",
    "    \n",
    "client.upload_model_package(artifact_path=model_dir, project_id=project_id, model_id=model_id)\n",
    "f\"Project '{project_id} now contains model '{model_id}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's trigger the model predictions by calling the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Successfully processed and uploaded predictions for dataset auto_insurance with model tutorial_gradient_boosting_regressor'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.trigger_model_predictions(project_id, model_id, dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test out our model by interfacing with the client and calling [run model](https://docs.fiddler.ai/api-reference/python-package/#run-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_customer_lifetime_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6353.814232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6935.858778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4736.136645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6399.515196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11618.909563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6902.586371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8120.247485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8618.783066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6260.395796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19947.630198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted_customer_lifetime_value\n",
       "0                        6353.814232\n",
       "1                        6935.858778\n",
       "2                        4736.136645\n",
       "3                        6399.515196\n",
       "4                       11618.909563\n",
       "5                        6902.586371\n",
       "6                        8120.247485\n",
       "7                        8618.783066\n",
       "8                        6260.395796\n",
       "9                       19947.630198"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_input = df_train[0: 10]\n",
    "result = client.run_model(project_id, model_id, prediction_input, log_events=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Seven: Simulate Monitoring Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will be simulating traffic to send for our model monitoring by using [publish_event](https://docs.fiddler.ai/api-reference/python-package/#publish-event). This will be the equivalent of running our model separately on some data, and either sending to Fiddler then, or saving this information to a log and sending at a later point.\n",
    "\n",
    "For this demonstration, we will be going with a log-related approach. This log will contain rows that have inputs, predictions, and real targets. To most accurately simulate this as a time-series event, we will also be calling a function to generate a timestamp in the last 2 weeks. Real data will ideally have a timestamp related to when the event took place; otherwise, the current time will be used.\n",
    "\n",
    "**Note**: The timestamp must be in UTC milliseconds. See [here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 50 / 50 \n",
      "2020-12-14 19:38:55.172000 UTC: \n",
      "{'customer': 'DE55857', 'location_state': 'Washington', 'customer_lifetime_value': 2465.444865, 'response': 'No', 'coverage': 'Basic', 'education': 'Bachelor', 'effective_to_date': '1/30/11', 'employmentstatus': 'Employed', 'gender': 'F', 'income': 64997, 'location_code': 'Suburban', 'marital_status': 'Single', 'monthly_premium_auto': 63, 'months_since_last_claim': 7, 'months_since_policy_inception': 99, 'number_of_open_complaints': 1, 'number_of_policies': 1, 'policy_type': 'Personal Auto', 'policy': 'Personal L3', 'renew_offer_type': 'Offer2', 'sales_channel': 'Agent', 'total_claim_amount': 383.442328, 'vehicle_class': 'Four-Door Car', 'vehicle_size': 'Small', 'predicted_customer_lifetime_value': 6956.451807764521, '__event_type': 'execution_event', '__occurred_at': 1607974735172}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from random import sample, randint\n",
    "NUM_EVENTS_TO_SEND = 50\n",
    "\n",
    "def getTimestampFromPastTwoWeeks():\n",
    "    \"\"\"\n",
    "    Generate a randomized timestamp from the past two weeks. Timestamp is in \n",
    "    milliseconds since epoch in UTC.\n",
    "    \"\"\"\n",
    "    TWO_WEEKS_MS = 604800 * 2 * 1000\n",
    "    current_time_in_ms = round(time.time() * 1000)\n",
    "    \n",
    "    random_time_in_past_two_weeks = current_time_in_ms - randint(0, TWO_WEEKS_MS)\n",
    "    return random_time_in_past_two_weeks\n",
    "\n",
    "        \n",
    "event_log = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/event_log.csv')\n",
    "\n",
    "# Convert this dataframe into a list of dictionary events, where each event is its own dictionary\n",
    "event_list_dict = event_log.sample(n=NUM_EVENTS_TO_SEND).to_dict(orient='records') \n",
    "\n",
    "for ind, event_dict in enumerate(event_list_dict):\n",
    "    event_ms_time_stamp = getTimestampFromPastTwoWeeks()\n",
    "    result = client.publish_event(project_id, model_id, event_dict, event_time_stamp=event_ms_time_stamp)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    readable_timestamp = datetime.datetime.fromtimestamp(event_ms_time_stamp/1000.0)\n",
    "    \n",
    "    print(f'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \\n{readable_timestamp} UTC: \\n{event_dict}')\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
