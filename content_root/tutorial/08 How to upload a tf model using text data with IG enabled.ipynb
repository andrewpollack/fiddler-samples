{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Fiddler Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/imdb_rnn/imdb_rnn.csv')\n",
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A real blow-up of the film literally. This Bri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I only wish that Return of the Jedi, have been...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"I like cheap perfume better; it doesn't last ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>On the eighth day God created Georges. But the...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No, this is not no Alice fairy tale my friends...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  polarity\n",
       "0  A real blow-up of the film literally. This Bri...     False\n",
       "1  I only wish that Return of the Jedi, have been...      True\n",
       "2  \"I like cheap perfume better; it doesn't last ...      True\n",
       "3  On the eighth day God created Georges. But the...      True\n",
       "4  No, this is not no Alice fairy tale my friends...      True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imdb_rnn' not in client.list_datasets():\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'train': df}, \n",
    "        dataset_id='imdb_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'polarity'\n",
    "feature_columns = ['sentence']\n",
    "train_input = df[feature_columns]\n",
    "train_target = df[target]\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info('imdb_rnn'),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    display_name='Text IG',\n",
    "    description='this is a tensorflow model using text data and IG enabled from tutorial',\n",
    "    input_type=fdl.ModelInputType.TEXT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_target = le.fit_transform(train_target)\n",
    "train_target = train_target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "vocab_size = 2000\n",
    "encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(train_input['sentence'],\n",
    "                                                                  target_vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def strip_accents_and_special_characters(s):\n",
    "    return unidecode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "input_tokens = (train_input['sentence']\n",
    "                .apply(lambda x: encoder.encode(\n",
    "                    strip_accents_and_special_characters(x))))\n",
    "\n",
    "max_seq_length = 300\n",
    "\n",
    "input_tokens_padd = sequence.pad_sequences(\n",
    "    input_tokens,\n",
    "    maxlen=max_seq_length,\n",
    "    padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs', shape=[max_seq_length])\n",
    "    layer = Embedding(vocab_size, 64, input_length=max_seq_length)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256, name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1, name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 300, 64)           128000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 177,921\n",
      "Trainable params: 177,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 35s 2ms/sample - loss: 0.6505 - acc: 0.6054 - val_loss: 0.5857 - val_acc: 0.7032\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 35s 2ms/sample - loss: 0.5007 - acc: 0.7824 - val_loss: 0.5490 - val_acc: 0.7664\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 35s 2ms/sample - loss: 0.4838 - acc: 0.7915 - val_loss: 0.4999 - val_acc: 0.7916\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 35s 2ms/sample - loss: 0.4333 - acc: 0.8262 - val_loss: 0.4251 - val_acc: 0.8268\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 34s 2ms/sample - loss: 0.4290 - acc: 0.8235 - val_loss: 0.4236 - val_acc: 0.8248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f861db7bd50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.fit(input_tokens_padd, train_target, batch_size=128, epochs=5,\n",
    "          validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leagenuit/opt/anaconda3/envs/fiddler/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Export includes no default signature!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Export includes no default signature!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Export includes no default signature!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Export includes no default signature!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: imdb_rnn_model/saved_model/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: imdb_rnn_model/saved_model/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import pickle\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "\n",
    "project_id = 'tutorial'\n",
    "model_id = 'imdb_rnn_model'\n",
    "\n",
    "# create temp dir\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save model\n",
    "tf.keras.experimental.export_saved_model(model, str(model_dir / 'saved_model'))\n",
    "\n",
    "# save model schema\n",
    "with open(model_dir / 'model.yaml', 'w') as yaml_file:\n",
    "    yaml.dump({'model': model_info.to_dict()}, yaml_file)\n",
    "\n",
    "# save tokenizer\n",
    "with open(model_dir / 'tokenizer.pkl', 'wb') as tok_file:\n",
    "    tok_file.write(pickle.dumps(encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write package.py and related wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing imdb_rnn_model/tf_saved_model_wrapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile imdb_rnn_model/tf_saved_model_wrapper.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "\n",
    "class TFSavedModelWrapper:\n",
    "    def __init__(self, saved_model_path, sig_def_key, output_columns,\n",
    "                 is_binary_classification=False, output_key=None,\n",
    "                 batch_size=8):\n",
    "        \"\"\"\n",
    "        Wrapper to load and run a TF model from a saved_model path.\n",
    "        Models must extend this class in their package.py, and override the\n",
    "        transform_input method.\n",
    "\n",
    "        Args:\n",
    "        :param saved_model_path: Path to the directory containing the TF\n",
    "            model in SavedModel format.\n",
    "            See: https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel\n",
    "\n",
    "        :param sig_def_key: Key for the specific SignatureDef to be used for\n",
    "            executing the model.\n",
    "            See: https://www.tensorflow.org/tfx/serving/signature_defs#signaturedef_structure\n",
    "\n",
    "        :param output_columns: List containing the names of the output\n",
    "            column(s) that corresponds to the output of the model. If the\n",
    "            model is a binary classification model then the number of output\n",
    "            columns is one, otherwise, the number of columns must match the\n",
    "            shape of the output tensor corresponding to the output key\n",
    "            specified.\n",
    "\n",
    "         :param is_binary_classification [optional]: Boolean specifying if the\n",
    "            model is a binary classification model. If True, the number of\n",
    "            output columns is one. The default is False.\n",
    "\n",
    "        :param output_key [optional]: Key for the specific output tensor (\n",
    "            specified in the SignatureDef) whose predictions must be explained.\n",
    "            The output tensor must specify a differentiable output of the\n",
    "            model. Thus, output tensors that are generated as a result of\n",
    "            discrete operations (e.g., argmax) are disallowed. The default is\n",
    "            None, in which case the first output listed in the SignatureDef is\n",
    "            used. The 'saved_model_cli' can be used to view the output tensor\n",
    "            keys available in the signature_def.\n",
    "            See: https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel\n",
    "\n",
    "        :param batch_size [optional]: the batch size for input into the model.\n",
    "            Depends on model and instance config.\n",
    "        \"\"\"\n",
    "\n",
    "        self.saved_model_path = saved_model_path\n",
    "        self.sig_def_key = sig_def_key\n",
    "        self.output_key = output_key\n",
    "        self.output_columns = output_columns\n",
    "        self.input_tensors = None\n",
    "        self.output_tensor = None\n",
    "        self.sess = None\n",
    "        self.saved_model = None\n",
    "        self.is_binary_classification = is_binary_classification\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the model and creates a session from the saved_model_path\n",
    "        provided at initialization.\n",
    "        \"\"\"\n",
    "        # load the model\n",
    "        self.sess = tf.Session()\n",
    "        self.saved_model = tf.saved_model.loader.load(\n",
    "            sess=self.sess, tags=['serve'],\n",
    "            export_dir=str(self.saved_model_path))\n",
    "\n",
    "        # Extract input and output tensors from the signature.\n",
    "        sig = self.saved_model.signature_def[self.sig_def_key]\n",
    "        self.input_tensors = sig.inputs\n",
    "\n",
    "        if self.output_key is None:\n",
    "            self.output_key = list(sig.outputs)[0]\n",
    "\n",
    "        self.output_tensor = self.get_tensor(sig.outputs[self.output_key].name)\n",
    "        if self.is_binary_classification:\n",
    "            if len(self.output_columns) != 1:\n",
    "                raise ValueError(f'Number of output columns should be one '\n",
    "                                 f'for a binary classification model, '\n",
    "                                 f'but length is {len(self.output_columns)} ')\n",
    "            # output_tensor should either be of shape <batch, > or <batch, 2>\n",
    "            output_tensor_shape = self.output_tensor.shape.as_list()\n",
    "            logging.info(f'Output tensor shape is {output_tensor_shape}')\n",
    "            if len(output_tensor_shape) == 2:\n",
    "                if output_tensor_shape[1] == 2:\n",
    "                    self.output_tensor = self.output_tensor[:, 1]\n",
    "\n",
    "    def transform_input(self, input_df):\n",
    "        raise NotImplementedError('Please implement transform_input in package.py')\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        \"\"\"\n",
    "        Returns predictions for the provided inputs.\n",
    "\n",
    "        Args:\n",
    "        :param input_df: DataFrame corresponding to the dataset yaml\n",
    "            associated with the project. Specifically, the columns in the\n",
    "            DataFrame must correspond to the feature names mentioned in the\n",
    "            yaml.\n",
    "\n",
    "        Returns:\n",
    "        - predictions_df: Pandas DataFrame with predictions for the provided\n",
    "            inputs. The columns of the DataFrame are the provided set of output\n",
    "            columns.\n",
    "        \"\"\"\n",
    "\n",
    "        transformed_input_df = self.transform_input(input_df)\n",
    "        predictions = []\n",
    "        for ind in range(0, len(transformed_input_df), self.batch_size):\n",
    "            df_chunk = transformed_input_df.iloc[ind: ind + self.batch_size]\n",
    "            feed = self.get_feed_dict(df_chunk)\n",
    "\n",
    "            with self.sess.as_default():\n",
    "                predictions += self.sess.run(self.output_tensor, feed).tolist()\n",
    "        return pd.DataFrame(predictions, columns=self.output_columns)\n",
    "\n",
    "    def get_tensor(self, name):\n",
    "        names = [t.name for t in self.sess.graph.as_graph_def().node]\n",
    "        try:\n",
    "            return self.sess.graph.get_tensor_by_name(name)\n",
    "        except:\n",
    "            raise ValueError(f'name: {name} not in graph: {names}')\n",
    "\n",
    "    def get_feed_dict(self, input_df):\n",
    "        \"\"\"\n",
    "        Returns the input dictionary to be fed to the TensorFlow graph given\n",
    "        input_df which is a pandas DataFrame. The input_df DataFrame is\n",
    "        obtained after applying transform_input on the raw input. The\n",
    "        transform_input function is extended in package.py.\n",
    "        \"\"\"\n",
    "\n",
    "        feed = {}\n",
    "        for key, tensor_info in self.input_tensors.items():\n",
    "            if key not in input_df.columns:\n",
    "                raise RuntimeError(f'Transformed input does not have a '\n",
    "                                   f'column corresponding to the input tensor '\n",
    "                                   f'key: {key} specified in the SignatureDef. Input col: {input_df.columns}')\n",
    "            feed_inp = input_df[key].tolist()\n",
    "            feed_inp_shape = np.array(feed_inp).shape\n",
    "            expected_shape = self.get_shape(tensor_info.tensor_shape)\n",
    "            if not self.match_shape(feed_inp_shape, expected_shape):\n",
    "                raise RuntimeError(f'Shape mismatch for input tensor {key}.'\n",
    "                                   f'Got: {feed_inp_shape}, Want '\n",
    "                                   f'{expected_shape}')\n",
    "            feed[tensor_info.name] = feed_inp\n",
    "        return feed\n",
    "\n",
    "    @staticmethod\n",
    "    def get_shape(tensor_shape):\n",
    "        \"\"\"\n",
    "        Returns shape of tensor having tensor shape in the format returned by\n",
    "        the SignatureDef\n",
    "        \"\"\"\n",
    "        return [d.size for d in tensor_shape.dim]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_shape_tensor(tensor_shape):\n",
    "        \"\"\"\n",
    "        Returns shape of tensor having tensor shape in the format of the\n",
    "        tf.TensorShape class\n",
    "        \"\"\"\n",
    "        return [d.value if d.value is not None else -1 for d in\n",
    "                tensor_shape.dims]\n",
    "\n",
    "    @staticmethod\n",
    "    def match_shape(got, want):\n",
    "        if len(got) != len(want):\n",
    "            return False\n",
    "        for i, v in enumerate(got):\n",
    "            if want[i] != -1 and want[i] != v and v != -1:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing imdb_rnn_model/tf_saved_model_wrapper_ig.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile imdb_rnn_model/tf_saved_model_wrapper_ig.py\n",
    "\n",
    "from .tf_saved_model_wrapper import TFSavedModelWrapper\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "\n",
    "class TFSavedModelWrapperIg(TFSavedModelWrapper):\n",
    "    def __init__(self, saved_model_path, sig_def_key, output_columns,\n",
    "                 is_binary_classification=False,\n",
    "                 output_key=None,\n",
    "                 batch_size=8,\n",
    "                 input_tensor_to_differentiable_layer_mapping={},\n",
    "                 max_allowed_error=None):\n",
    "        \"\"\"\n",
    "        Wrapper to support Integrated Gradients (IG) computation for a TF\n",
    "        model loaded from a saved_model path.\n",
    "\n",
    "        See: https://github.com/ankurtaly/Integrated-Gradients\n",
    "\n",
    "        Models must extend this class in their  package.py, and override the\n",
    "        transform_input and the project_attributions methods.\n",
    "\n",
    "        Args:\n",
    "        :param input_tensor_to_differentiable_layer_mapping [optional]:\n",
    "            Dictionary that maps input tensors to the first differentiable\n",
    "            layer/tensor in the graph they are attached to. For instance,\n",
    "            in a text model, an input tensor containing token ids\n",
    "            may not be differentiable but may feed into an embedding tensor.\n",
    "            Such an input tensor must be mapped to the corresponding the\n",
    "            embedding tensor in this dictionary.\n",
    "\n",
    "            All input tensors must be mentioned in the dictionary. An input\n",
    "            tensor that is directly differentiable may be mapped to itself.\n",
    "\n",
    "            For each differentiable tensor, the first dimension must be the\n",
    "            batch dimension. If <k1, …, kn> is the shape of the input then the\n",
    "            differentiable tensor must either have the same shape or the shape\n",
    "            <k1, …, kn, d>.\n",
    "\n",
    "            The default is None, in which case all input tensors are assumed\n",
    "            to be differentiable.\n",
    "\n",
    "        :param max_allowed_error: Float specifying a percentage value\n",
    "            for the maximum allowed integral approximation error for IG\n",
    "            computation. If None then IG will be  calculated for a\n",
    "            pre-determined number of steps. Otherwise, the number of steps\n",
    "            will be increased till the error is within the specified limit.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(saved_model_path, sig_def_key,\n",
    "                         output_columns=output_columns,\n",
    "                         is_binary_classification=is_binary_classification,\n",
    "                         output_key=output_key,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "        self.input_tensor_to_differentiable_layer_mapping = \\\n",
    "            input_tensor_to_differentiable_layer_mapping\n",
    "\n",
    "        # mapping from each input tensor to its differentiable version\n",
    "        self.differentiable_tensors = {}\n",
    "\n",
    "        # mapping each output column to a dictionary of gradients tensors.\n",
    "        self.gradient_tensors = {}\n",
    "        self.steps = 10  # no of steps for ig calculation\n",
    "        self.ig_enabled = True  #\n",
    "        self.max_allowed_error = max_allowed_error\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Extends load model defined in the TFSavedModelWrapper class\"\"\"\n",
    "        super().load_model()\n",
    "\n",
    "        for key, tensor_info in self.input_tensors.items():\n",
    "            if key in self.input_tensor_to_differentiable_layer_mapping.keys():\n",
    "                differentiable_tensor = \\\n",
    "                    self.get_tensor(\n",
    "                        self.input_tensor_to_differentiable_layer_mapping[key])\n",
    "                # shape check\n",
    "                diff_tensor_shape = \\\n",
    "                    self.get_shape_tensor(differentiable_tensor.shape)\n",
    "                input_tensor_shape = self.get_shape(tensor_info.tensor_shape)\n",
    "\n",
    "                logging.info(f'For key {key} differentiable tensor shape is '\n",
    "                             f'{diff_tensor_shape} input tensor shape is '\n",
    "                             f'{input_tensor_shape}')\n",
    "                if self._validate_differentiable_tensor_shape(\n",
    "                        diff_tensor_shape, input_tensor_shape):\n",
    "                    self.differentiable_tensors[key] = \\\n",
    "                        differentiable_tensor\n",
    "                else:\n",
    "                    raise ValueError(f'Shape of differentiable tensor '\n",
    "                                     f'{diff_tensor_shape} doesnt follow rule '\n",
    "                                     f'\"If <k1, …, kn> is the shape of the '\n",
    "                                     f'input then the differentiable tensor '\n",
    "                                     f'must either have the same shape or the '\n",
    "                                     f'shape <k1, …, kn, d>\". Shape of input '\n",
    "                                     f'tensor is {input_tensor_shape}')\n",
    "\n",
    "        if self.is_binary_classification:\n",
    "            self.gradient_tensors[self.output_columns[0]] = {}\n",
    "            for key, tensor in self.differentiable_tensors.items():\n",
    "                self.gradient_tensors[self.output_columns[0]][key] = \\\n",
    "                    tf.gradients(self.output_tensor, tensor)\n",
    "        else:\n",
    "            for index, column in enumerate(self.output_columns):\n",
    "                self.gradient_tensors[column] = {}\n",
    "                for key, tensor in self.differentiable_tensors.items():\n",
    "                    self.gradient_tensors[column][key] = \\\n",
    "                        tf.gradients(self.output_tensor[:, index], tensor)\n",
    "\n",
    "    def generate_baseline(self, input_df):\n",
    "        raise NotImplementedError('Please implement generate_baseline in '\n",
    "                                  'package.py')\n",
    "\n",
    "    def project_attributions(self, input_df, transformed_input_df,\n",
    "                             attributions):\n",
    "        raise NotImplementedError('Please implement project_attributions in '\n",
    "                                  'package.py')\n",
    "\n",
    "    def _validate_differentiable_tensor_shape(self,\n",
    "                                              differentiable_tensor_shape,\n",
    "                                              input_tensor_shape):\n",
    "\n",
    "        diff_len = len(differentiable_tensor_shape)\n",
    "        input_len = len(input_tensor_shape)\n",
    "        if diff_len == input_len:\n",
    "            return self.match_shape(differentiable_tensor_shape,\n",
    "                                    input_tensor_shape)\n",
    "        elif diff_len - input_len == 1:\n",
    "            return self.match_shape(differentiable_tensor_shape[:-1],\n",
    "                                    input_tensor_shape)\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing imdb_rnn_model/cover_tokens.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile imdb_rnn_model/cover_tokens.py\n",
    "\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def strip_accents_and_special_characters(s):\n",
    "    return unidecode(s)\n",
    "\n",
    "\n",
    "def one_split(in_strings: list,\n",
    "              split_string: str,\n",
    "              strip_whitespace: bool) -> list:\n",
    "    \"\"\"Break each string in a list of strings into smaller parts.\n",
    "\n",
    "    Split after each occurrence of split_string.\n",
    "\n",
    "    :param in_strings: List of strings to be broken into substrings.\n",
    "    :param split_string: A separator string after which to divide.\n",
    "    :param strip_whitespace:(bool) leading/trailing whitespace from\n",
    "        tokens.\n",
    "    :return: A list of (probably) smaller strings.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    include_delim = split_string.strip() if strip_whitespace else split_string\n",
    "\n",
    "    for sub_str in in_strings:\n",
    "        splits = sub_str.split(split_string)\n",
    "        for piece in splits:\n",
    "            if strip_whitespace:\n",
    "                piece = piece.strip()\n",
    "            if piece:\n",
    "                out.append(piece)\n",
    "            if include_delim:\n",
    "                out.append(split_string)\n",
    "        if include_delim:\n",
    "            out.pop()\n",
    "    return out\n",
    "\n",
    "\n",
    "def multi_split(in_strings: str,\n",
    "                split_strings: tuple,\n",
    "                strip_whitespace: bool) -> list:\n",
    "    \"\"\"\n",
    "    Split strings in a list at any of multiple split-strings.\n",
    "\n",
    "    :param in_strings:  List of strings to be broken into substrings.\n",
    "    :param split_strings: List of string separators after which to\n",
    "        divide.\n",
    "    :param strip_whitespace: Remove leading/trailing whitespace from\n",
    "        tokens?\n",
    "    :return: A list of (probably) smaller strings.\n",
    "    \"\"\"\n",
    "    out = [in_strings]\n",
    "    for split_string in split_strings:\n",
    "        out = one_split(out, split_string, strip_whitespace)\n",
    "    return out\n",
    "\n",
    "\n",
    "def word_tokenizer(raw_string: str,\n",
    "                   delimiters: tuple =\n",
    "                   (' ', '.', ',', '>', '!', ';', ':', '--'),\n",
    "                   strip_whitespace: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Simple tokenizer that splits on spaces and assorted punctuation.\n",
    "    Also retains separators.\n",
    "\n",
    "    :param raw_string: string to tokenize\n",
    "    :param delimiters: [(' ', '.', ',', '>', '!', ';', ':', '--')]\n",
    "        list of string splitting delimiters.\n",
    "    :param strip_whitespace: [False] Remove leading/trailing whitespace\n",
    "        from tokens?\n",
    "    :return: List of substrings\n",
    "    \"\"\"\n",
    "    return multi_split(raw_string, delimiters, strip_whitespace)\n",
    "\n",
    "\n",
    "def cover_tokens(coarse_grained_tokens: list,\n",
    "                 fine_grained_tokens: list,\n",
    "                 num_fine_tokens_to_be_matched=None) -> list:\n",
    "    \"\"\"\n",
    "    Given two tokenizations of a sentence -- one coarse-grained (e.g.\n",
    "    word-level tokenization), and one fine-grained (e.g., wordpiece-\n",
    "    level tokenization), this method returns a covering of the\n",
    "    coarse-grained tokens with fine-grained tokens.\n",
    "\n",
    "    Specifically, the returned covering comes with the guarantee that\n",
    "    the concatenation of the lists of fine-grained tokens assigned to\n",
    "    each coarse-grained token would recover the original list of\n",
    "    fine-grained tokens.\n",
    "\n",
    "    Additionally, the fine-grained tokens may include additional\n",
    "    characters (which some tokenizers create), but MUST CONTAIN all the\n",
    "    characters from the concatenated coarse-grained tokens in the same\n",
    "    order (until num_fine_tokens_to_be_matched fine-tokens have been\n",
    "    processed, if specified).\n",
    "\n",
    "    Further, a fine-grained token is guaranteed to belong to one and\n",
    "    only one coarse-grained token and is associated with the first\n",
    "    coarse-grained token to which it contributes.  It doesn't need to\n",
    "    end in the same coarse-grained token... this helps to accommodate\n",
    "    tokenizers that may split whitespace and punctuation differently.\n",
    "\n",
    "    The method returns None if a suitable covering cannot be defined.\n",
    "\n",
    "    Example:\n",
    "\n",
    "      sentence = 'coarse tokens fine.'\n",
    "\n",
    "      coarse = word_tokenizer(sentence)\n",
    "      # ['coarse', ' ', 'tokens', ' ', 'fine', '.']\n",
    "\n",
    "      fine = imdb_rnn_tokenizer(sentence)\n",
    "      # ['coa', 'rse', ' ', 'to', 'ken', 's ', 'fine', '.']\n",
    "\n",
    "      cover_tokens_new(coarse, fine)\n",
    "\n",
    "      # [('coarse', ['coa', 'rse']),\n",
    "      #  (' ', [' ']),\n",
    "      #  ('tokens', ['to', 'ken', 's ']),\n",
    "      #  (' ', []),\n",
    "      #  ('fine', ['fine']),\n",
    "      #  ('.', ['.'])]\n",
    "\n",
    "    Notice that 's ' in the fine tokenization straddles two coarse-\n",
    "    tokens 'tokens' and ' ', it is associated with the first, but the\n",
    "    still satisfies the requirement that the second is character-for-\n",
    "    character matched.\n",
    "\n",
    "    :param coarse_grained_tokens: List with tokens from a\n",
    "    coarse-grained tokenization (e.g., word-level tokenization) of the\n",
    "    input sentence\n",
    "\n",
    "    :param fine_grained_tokens: List with tokens from a\n",
    "    fine-grained tokenization (e.g., wordpiece-level or character-level\n",
    "    tokenization) of the  input sentence.\n",
    "\n",
    "    :param num_fine_tokens_to_be_matched: [Default None] If None,\n",
    "    require all characters in coarse-grained tokens to be matched.\n",
    "    If this is passed an integer, only require this many fine-grained\n",
    "    tokens to match before declaring the covering valid. Helpful when\n",
    "    model takes a specific number of input tokens.\n",
    "\n",
    "    :returns token_covering: List of tuples where the i^th tuple\n",
    "    consists of the i^th coarse-grained token followed by a list of\n",
    "    fine-grained tokens it maps to; None if covering isn't possible.\n",
    "    \"\"\"\n",
    "    coverings = []\n",
    "    num_fine_tokens_processed = 0\n",
    "\n",
    "    fine_token_iter = iter(fine_grained_tokens)\n",
    "    coarse_token_iter = iter(coarse_grained_tokens)\n",
    "\n",
    "    # These init values will kick-off the draw loops for coarse and fine chars\n",
    "    coarse_char_iter = iter('')\n",
    "    fine_char_iter = iter('')\n",
    "    coarse_char = None\n",
    "    fine_char = None\n",
    "\n",
    "    # This while loop compares coarse and fine tokens, one character at a time\n",
    "    # If they match, both step; if not, only the fine character steps this\n",
    "    # allows the algorithm to skip extra characters that the fine tokenizer\n",
    "    # might have added.\n",
    "    #\n",
    "    # Each time a new fine-grained token is drawn, it is added to the current\n",
    "    # active coarse grained token right away.\n",
    "    #\n",
    "    # If a coarse-grained token ins consumed, a new one is picked up and a new\n",
    "    # covering entry is created for it.  Any partially processed fine tokens\n",
    "    # will continue to match characters in the new coarse token.  However\n",
    "    # the fine-token will continue to be associated with only the previous\n",
    "    # covering.  See the docstring example for a straddling case with the\n",
    "    # 's ' fine-grained token.\n",
    "\n",
    "    while True:\n",
    "        if coarse_char == fine_char:\n",
    "            while True:  # Draw coarse_char until valid\n",
    "                try:\n",
    "                    coarse_char = next(coarse_char_iter)\n",
    "                    break\n",
    "                except StopIteration:  # Need a new token\n",
    "                    try:\n",
    "                        coarse_token = next(coarse_token_iter)\n",
    "                        coverings.append([coarse_token, []])\n",
    "                        coarse_char_iter = iter(coarse_token)\n",
    "                    except StopIteration:  # End of coarse tokens\n",
    "                        return coverings\n",
    "\n",
    "        # Increment fine_char whether or not there was a match.\n",
    "        while True:  # Draw fine_char until valid\n",
    "            try:\n",
    "                fine_char = next(fine_char_iter)\n",
    "                break\n",
    "            except StopIteration:  # Need a new token\n",
    "                try:\n",
    "                    fine_token = next(fine_token_iter)\n",
    "                    num_fine_tokens_processed += 1\n",
    "                    coverings[-1][1].append(fine_token)\n",
    "                    fine_char_iter = iter(fine_token)\n",
    "                except StopIteration:  # End of fine tokens\n",
    "                    if (num_fine_tokens_to_be_matched and\n",
    "                            num_fine_tokens_processed >=\n",
    "                            num_fine_tokens_to_be_matched):\n",
    "                        return coverings\n",
    "                    else:\n",
    "                        return None\n",
    "\n",
    "\n",
    "def regroup_attributions(coverings: list, fine_attributions: list) -> list:\n",
    "    \"\"\"\n",
    "    Produces a list of len(coverings) summed attributions according to\n",
    "    the groupings of the tuples in coverings.\n",
    "\n",
    "     Example:\n",
    "\n",
    "       covering =[(“simple”, [“simple”]),\n",
    "                  (“example”, [“exam#”, “#ple”])]\n",
    "\n",
    "       fine_attributions = [0.1, 0.3. 0.4]\n",
    "\n",
    "       regroup_attributions(covering, fine_attributions)\n",
    "\n",
    "       # [ 0.1,  0.7 ] <- one fore each coarse token\n",
    "\n",
    "    :param coverings: List of tuples grouping tokens together\n",
    "    :param fine_attributions: List of attribution values, one for\n",
    "        each entry in the concatenated covering list.\n",
    "    :return: A list of combined coverings, one for each tuple in\n",
    "    covering.\n",
    "    \"\"\"\n",
    "    coarse_attributions = []\n",
    "    offset = 0\n",
    "\n",
    "    for coarse_token, fine_tokens_covered in coverings:\n",
    "        num_tokens = len(fine_tokens_covered)\n",
    "        coarse_attributions.append(sum(fine_attributions[\n",
    "                                       offset:offset + num_tokens])\n",
    "                                   if num_tokens else 0.)\n",
    "        offset += num_tokens\n",
    "    return coarse_attributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting imdb_rnn_model/package.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile imdb_rnn_model/package.py\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from .cover_tokens import strip_accents_and_special_characters\n",
    "from .cover_tokens import word_tokenizer\n",
    "from .cover_tokens import cover_tokens\n",
    "from .cover_tokens import regroup_attributions\n",
    "from .tf_saved_model_wrapper_ig import TFSavedModelWrapperIg\n",
    "\n",
    "\n",
    "PACKAGE_PATH = pathlib.Path(__file__).parent\n",
    "SAVED_MODEL_PATH = PACKAGE_PATH / 'saved_model'\n",
    "TOKENIZER_PATH = PACKAGE_PATH / 'tokenizer.pkl'\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MyModel(TFSavedModelWrapperIg):\n",
    "    def __init__(self, saved_model_path, sig_def_key, tokenizer_path,\n",
    "                 is_binary_classification=False,\n",
    "                 output_key=None,\n",
    "                 batch_size=8,\n",
    "                 output_columns=[],\n",
    "                 input_tensor_to_differentiable_layer_mapping={},\n",
    "                 max_allowed_error=None):\n",
    "        \"\"\"\n",
    "        Class to load and run the IMDB RNN model.\n",
    "        See: TFSavedModelWrapper\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(saved_model_path, sig_def_key,\n",
    "                         is_binary_classification=is_binary_classification,\n",
    "                         output_key=output_key,\n",
    "                         batch_size=batch_size,\n",
    "                         output_columns=output_columns,\n",
    "                         input_tensor_to_differentiable_layer_mapping=\n",
    "                         input_tensor_to_differentiable_layer_mapping,\n",
    "                         max_allowed_error=max_allowed_error)\n",
    "        with open(tokenizer_path, 'rb') as handle:\n",
    "            self.tokenizer = pickle.load(handle)\n",
    "        self.max_seq_length = 300\n",
    "\n",
    "    def transform_input(self, input_df):\n",
    "        \"\"\"\n",
    "        Transform the provided dataframe into one that complies with the input\n",
    "        interface of the model.\n",
    "\n",
    "        Overrides the transform_input method of TFSavedModelWrapper.\n",
    "        \"\"\"\n",
    "\n",
    "        input_tokens = (input_df['sentence']\n",
    "                        .apply(lambda x: self.tokenizer.encode(\n",
    "                                strip_accents_and_special_characters(x))))\n",
    "\n",
    "        input_tokens = sequence.pad_sequences(input_tokens,\n",
    "                                              maxlen=self.max_seq_length,\n",
    "                                              padding=\"post\"\n",
    "                                             )\n",
    "\n",
    "        return pd.DataFrame({'inputs': input_tokens.tolist()})\n",
    "\n",
    "    def generate_baseline(self, input_df):\n",
    "\n",
    "        input_tokens = input_df['sentence'].apply(lambda x:\n",
    "                                                  self.tokenizer.encode(''))\n",
    "        input_tokens = sequence.pad_sequences(input_tokens,\n",
    "                                              maxlen=self.max_seq_length,\n",
    "                                              padding=\"post\"\n",
    "                                             )\n",
    "\n",
    "        return pd.DataFrame({'inputs': input_tokens.tolist()})\n",
    "\n",
    "    def project_attributions(self, input_df, transformed_input_df,\n",
    "                             attributions):\n",
    "        \"\"\"\n",
    "        Maps the transformed input to original input space so that the\n",
    "        attributions correspond to the features of the original input.\n",
    "        Overrides the project_attributions method of TFSavedModelWrapper.\n",
    "        \"\"\"\n",
    "        \n",
    "        wordpiece_tokens = [self.tokenizer.decode([int(t)]) for t in\n",
    "                            (transformed_input_df['inputs'][0])]\n",
    "\n",
    "        word_tokens = word_tokenizer(\n",
    "            strip_accents_and_special_characters(\n",
    "                input_df['sentence'].iloc[0]))\n",
    "\n",
    "        coverings = cover_tokens(word_tokens,\n",
    "                                 wordpiece_tokens,\n",
    "                                 num_fine_tokens_to_be_matched=\n",
    "                                 self.max_seq_length)\n",
    "\n",
    "        word_attributions = regroup_attributions(\n",
    "            coverings,\n",
    "            attributions['inputs'][0].astype(\n",
    "                'float').tolist())\n",
    "        if word_attributions:\n",
    "            return {'embedding_input': [word_tokens, word_attributions]}\n",
    "        else:\n",
    "            LOG.info('Cover tokens failed.  Falling back to wordpiece tokens')\n",
    "            return {'embedding_input': [wordpiece_tokens,\n",
    "                                        attributions['inputs'\n",
    "                                                     ][0].astype(\n",
    "                                                     'float').tolist()\n",
    "                                        ]}\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = MyModel(\n",
    "        SAVED_MODEL_PATH,\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n",
    "        TOKENIZER_PATH,\n",
    "        is_binary_classification=True,\n",
    "        batch_size=128,\n",
    "        output_columns=['inputs'],\n",
    "        input_tensor_to_differentiable_layer_mapping=\n",
    "        {'inputs': 'embedding/embedding_lookup:0'},\n",
    "        max_allowed_error=5)\n",
    "    model.load_model()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_model(project_id, model_id)\n",
    "client.upload_model_package(model_dir, project_id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.148647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.959517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.603902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.951737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.909023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.151743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.105851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.150470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.140991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     inputs\n",
       "0  0.148647\n",
       "1  0.959517\n",
       "2  0.155386\n",
       "3  0.603902\n",
       "4  0.951737\n",
       "5  0.909023\n",
       "6  0.151743\n",
       "7  0.105851\n",
       "8  0.150470\n",
       "9  0.140991"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_input = train_input[:10]\n",
    "result = client.run_model(project_id, model_id, prediction_input)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_point = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tutorial'\n",
    "model_id = 'imdb_rnn_model'\n",
    "\n",
    "ex_ig = client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    df=selected_point, \n",
    "    dataset_id='imdb_rnn',\n",
    "    explanations='ig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
